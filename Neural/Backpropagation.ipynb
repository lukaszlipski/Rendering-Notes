{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cd7f53",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "### Basic neuron formulas\n",
    "$$ E = { 1 \\over 2 } \\sum_{i=1} (y_i - a_i)^2 $$\n",
    "$$ a_i^L = \\sigma(z_i^L) $$\n",
    "$$ z_i^L = \\sum_{j=1}^K ( w_{i,j} * a_j^{L-1} + b_i^L ) $$\n",
    "\n",
    "where:\n",
    "- $ L $ - current layer index\n",
    "- $ (L-1) $ - previous layer index\n",
    "- $ E $ - error function\n",
    "- $ a_i $ - activation function\n",
    "- $ z_i $ - linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fc023",
   "metadata": {},
   "source": [
    "### Calculating weight and bias derivatives for the last layer\n",
    "\n",
    "Using the chain rule, we can calculate the derivatives of the weights and biases for the last layer:\n",
    "$$ { \\partial E \\over \\partial w_{i,j}^L } = { \\partial E \\over \\partial a_i } { \\partial a_i \\over \\partial z_i } { \\partial z_i \\over \\partial w_{i,j} } $$\n",
    "$$ { \\partial E \\over \\partial b_i^L } = { \\partial E \\over \\partial a_i } { \\partial a_i \\over \\partial z_i } { \\partial z_i \\over \\partial b_i } $$\n",
    "\n",
    "where:\n",
    "- $ { \\partial E \\over \\partial a_i^L } = y_i - a_i^L $\n",
    "- $ { \\partial a_i^L \\over \\partial z_i^L } $ - depends on the $ \\sigma $\n",
    "- $ { \\partial z_i^L \\over \\partial w_{i,j}^L } = a_j^{L-1} $\n",
    "- $ { \\partial z_i^L \\over \\partial b_i^L } = 1 $\n",
    "\n",
    "Example individual weights (assuming 2 output, 3 hidden nodes)\n",
    "$$ \\begin{bmatrix}\n",
    "{ \\partial E \\over \\partial w_{0,0}^L } &\n",
    "{ \\partial E \\over \\partial w_{0,1}^L } &\n",
    "{ \\partial E \\over \\partial w_{0,2}^L } \\\\\n",
    "{ \\partial E \\over \\partial w_{1,0}^L } &\n",
    "{ \\partial E \\over \\partial w_{1,1}^L } &\n",
    "{ \\partial E \\over \\partial w_{1,2}^L }\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "(y_0 - a_0^L) ({ \\partial a_0^L \\over \\partial z_0^L }) a_0^{L-1} &\n",
    "(y_0 - a_0^L) ({ \\partial a_0^L \\over \\partial z_0^L }) a_1^{L-1} &\n",
    "(y_0 - a_0^L) ({ \\partial a_0^L \\over \\partial z_0^L }) a_2^{L-1} \\\\\n",
    "(y_1 - a_1^L) ({ \\partial a_1^L \\over \\partial z_1^L }) a_0^{L-1} &\n",
    "(y_1 - a_1^L) ({ \\partial a_1^L \\over \\partial z_1^L }) a_1^{L-1} &\n",
    "(y_1 - a_1^L) ({ \\partial a_1^L \\over \\partial z_1^L }) a_2^{L-1} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Concise implementation for bias and weight gradient using outer product:\n",
    "$$ \n",
    "W^L =\n",
    "\\begin{bmatrix}\n",
    "(y_0 - a_0^L) { \\partial a_0^L \\over \\partial z_0^L } \\\\\n",
    "(y_1 - a_1^L) { \\partial a_1^L \\over \\partial z_1^L }\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "a_0^{L-1} \\\\\n",
    "a_1^{L-1} \\\\\n",
    "a_2^{L-1}\n",
    "\\end{bmatrix}^T $$\n",
    "\n",
    "$$ \n",
    "b^L =\n",
    "\\begin{bmatrix}\n",
    "(y_0 - a_0^L) { \\partial a_0^L \\over \\partial z_0^L } \\\\\n",
    "(y_1 - a_1^L) { \\partial a_1^L \\over \\partial z_1^L }\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7041, 3.5989])\n",
      "tensor([[0.3509, 0.0000, 1.3461],\n",
      "        [0.7411, 0.0000, 2.8430]])\n"
     ]
    }
   ],
   "source": [
    "# Code example for manually calculating weight gradients for the last layer\n",
    "import torch\n",
    "\n",
    "# output layer\n",
    "output = torch.relu(torch.randn(2)) # 2 output neurons\n",
    "label = torch.randn(2) # grand truth labels\n",
    "sigma_derivative = (output > 0).to(torch.float32) # ReLU derivative\n",
    "\n",
    "# hidden layer\n",
    "hidden_activation = torch.relu(torch.randn(3)) # 3 hidden neurons\n",
    "\n",
    "# weight and bias gradients\n",
    "bias_grad = (output - label) * sigma_derivative\n",
    "weight_grad = torch.outer(bias_grad, hidden_activation)\n",
    "print(bias_grad)\n",
    "print(weight_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a40e98",
   "metadata": {},
   "source": [
    "### Calculating weight and bias derivatives for the last second to last layer\n",
    "\n",
    "$$ { \\partial E \\over \\partial w_{i,j}^{L-1} } = \\sum_k{ ({ \\partial E \\over \\partial a_k^L } { \\partial a_k^L \\over \\partial z_k^L } { \\partial z_k^L \\over \\partial a_i^{L-1} }) } { \\partial a_i^{L-1} \\over \\partial z_i^{L-1} } { \\partial z_i^{L-1} \\over \\partial w_{i,j}^{L-1} } $$\n",
    "$$ { \\partial E \\over \\partial b_i^{L-1} } = \\sum_k{ ({ \\partial E \\over \\partial a_k^L } { \\partial a_k^L \\over \\partial z_k^L } { \\partial z_k^L \\over \\partial a_i^{L-1} }) } { \\partial a_i^{L-1} \\over \\partial z_i^{L-1} } { \\partial z_i^{L-1} \\over \\partial b_i^{L-1} } $$\n",
    "\n",
    "where:\n",
    "- $ { \\partial z_k \\over \\partial a_i^{L-1} } = w_{k,i}^L $\n",
    "- $ { \\partial a_i^{L-1} \\over \\partial z_i^{L-1} } $ - depends on the $ \\sigma $\n",
    "- $ { \\partial z_i^{L-1} \\over \\partial w_{i,j}^{L-1} } = a_j^{L-2} $\n",
    "- $ { \\partial z_i^{L-1} \\over \\partial b_i^{L-1} } = 1 $\n",
    "\n",
    "Example individual weights (assuming 3 hidden nodes in $L-1$, 2 hidden nodes in $ L - 2 $) and $k$ is the number of output nodes:\n",
    "$$ \\begin{bmatrix}\n",
    "{ \\partial E \\over \\partial w_{0,0}^{L-1} } &\n",
    "{ \\partial E \\over \\partial w_{0,1}^{L-1} } \\\\\n",
    "{ \\partial E \\over \\partial w_{1,0}^{L-1} } &\n",
    "{ \\partial E \\over \\partial w_{1,1}^{L-1} } \\\\\n",
    "{ \\partial E \\over \\partial w_{2,0}^{L-1} } &\n",
    "{ \\partial E \\over \\partial w_{2,1}^{L-1} }\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "(\\sum_k{ ( y_k - a_k^L ) {\\partial a_k^N \\over \\partial z_k^N} w_{k,0}^N }) {\\partial a_0^{L-1} \\over \\partial z_0^{L-1} } a_0^{L-2}  &\n",
    "(\\sum_k{ ( y_k - a_k^L ) {\\partial a_k^N \\over \\partial z_k^N} w_{k,0}^N }) {\\partial a_0^{L-1} \\over \\partial z_0^{L-1} } a_1^{L-2} \\\\\n",
    "(\\sum_k{ ( y_k - a_k^L ) {\\partial a_k^N \\over \\partial z_k^N} w_{k,1}^N }) {\\partial a_1^{L-1} \\over \\partial z_1^{L-1} } a_0^{L-2} &\n",
    "(\\sum_k{ ( y_k - a_k^L ) {\\partial a_k^N \\over \\partial z_k^N} w_{k,1}^N }) {\\partial a_1^{L-1} \\over \\partial z_1^{L-1} } a_1^{L-2} \\\\\n",
    "(\\sum_k{ ( y_k - a_k^L ) {\\partial a_k^N \\over \\partial z_k^N} w_{k,2}^N }) {\\partial a_2^{L-1} \\over \\partial z_2^{L-1} } a_0^{L-2} &\n",
    "(\\sum_k{ ( y_k - a_k^L ) {\\partial a_k^N \\over \\partial z_k^N} w_{k,2}^N }) {\\partial a_2^{L-1} \\over \\partial z_2^{L-1} } a_1^{L-2} \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Concise implementation for bias and weight gradient using outer product:\n",
    "$$ \n",
    "W^{L-1} =\n",
    "(\n",
    "(W^L)^T\n",
    "\\begin{bmatrix}\n",
    "(y_0 - a_0^L) { \\partial a_0^L \\over \\partial z_0^L } \\\\\n",
    "(y_1 - a_1^L) { \\partial a_1^L \\over \\partial z_1^L }\n",
    "\\end{bmatrix}\n",
    ")\n",
    "{\\partial a_0^{L-1} \\over \\partial z_0^{L-1} }\n",
    "\\otimes\n",
    "\\begin{bmatrix}\n",
    "a_0^{L-2} \\\\\n",
    "a_1^{L-2}\n",
    "\\end{bmatrix}^T $$\n",
    "\n",
    "$$ \n",
    "b^{L-1} =\n",
    "(\n",
    "(W^L)^T\n",
    "\\begin{bmatrix}\n",
    "(y_0 - a_0^L) { \\partial a_0^L \\over \\partial z_0^L } \\\\\n",
    "(y_1 - a_1^L) { \\partial a_1^L \\over \\partial z_1^L }\n",
    "\\end{bmatrix}\n",
    ")\n",
    "{\\partial a_0^{L-1} \\over \\partial z_0^{L-1} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e264dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8434, -0.1364, -0.0000])\n",
      "tensor([[ 0.6257,  0.4763],\n",
      "        [-0.1012, -0.0770],\n",
      "        [-0.0000, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Code example for manually calculating weight gradients for the second last layer\n",
    "import torch\n",
    "\n",
    "# output layer\n",
    "output = torch.relu(torch.randn(2)) # 2 output neurons\n",
    "label = torch.randn(2) # grand truth labels\n",
    "output_sigma_derivative = (output > 0).to(torch.float32) # ReLU derivative\n",
    "\n",
    "# hidden layer\n",
    "hidden_activation = torch.relu(torch.randn(3)) # 3 hidden neurons\n",
    "hidden_sigma_derivative = (hidden_activation > 0).to(torch.float32) # ReLU derivative\n",
    "hidden_weight = torch.randn(2, 3) # weights from hidden to output layer\n",
    "\n",
    "# previous hidden layer\n",
    "prev_hidden_activation = torch.relu(torch.randn(2)) # 3 hidden neurons\n",
    "\n",
    "# weight and bias gradients\n",
    "bias_grad = (hidden_weight.T @ ((output - label) * output_sigma_derivative)) * hidden_sigma_derivative\n",
    "weight_grad = torch.outer(bias_grad, prev_hidden_activation)\n",
    "print(bias_grad)\n",
    "print(weight_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe708b",
   "metadata": {},
   "source": [
    "### Generic formula for Nth layer\n",
    "\n",
    "$$ \n",
    "b^{N} =\n",
    "(\n",
    "(W^{N+1})^T\n",
    "{\\partial E \\over \\partial b^{N+1}}\n",
    ")\n",
    "{\\partial a_0^{N} \\over \\partial z_0^{N} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "W^{N} =\n",
    "(\n",
    "b^{N}\n",
    ")\n",
    "\\otimes\n",
    "(a^{N-1})^T\n",
    "$$\n",
    "\n",
    "NOTE: We can reuse calculations for the current layer's bias during weight's calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weight grad difference: 0.0\n",
      "Layer 1 bias grad difference: 0.0\n",
      "Layer 2 weight grad difference: 0.0\n",
      "Layer 2 bias grad difference: 0.0\n",
      "Layer 3 weight grad difference: 0.0\n",
      "Layer 3 bias grad difference: 0.0\n",
      "Layer 4 weight grad difference: 0.0\n",
      "Layer 4 bias grad difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example of backpropagation step manually vs using autograd\n",
    "# NN model: \n",
    "#   * input (2 neurons)\n",
    "#   * 3x hidden layer (3,2,3 neurons) - ReLU activation\n",
    "#   * output (2 neurons)\n",
    "import torch\n",
    "\n",
    "# Define model parameters and input\n",
    "torch.manual_seed(1337)\n",
    "input = torch.tensor([0.5, -0.2])\n",
    "\n",
    "weights = [\n",
    "    torch.randn(3, 2, requires_grad=True), # input to hidden1\n",
    "    torch.randn(2, 3, requires_grad=True), # hidden1 to hidden2\n",
    "    torch.randn(3, 2, requires_grad=True), # hidden2 to hidden3\n",
    "    torch.randn(2, 3, requires_grad=True), # hidden3 to output\n",
    "]\n",
    "\n",
    "biases = [\n",
    "    torch.randn(3, requires_grad=True), # hidden1\n",
    "    torch.randn(2, requires_grad=True), # hidden2\n",
    "    torch.randn(3, requires_grad=True), # hidden3\n",
    "    torch.randn(2, requires_grad=True), # output\n",
    "]\n",
    "\n",
    "label = torch.tensor([1.0, 0.0])\n",
    "\n",
    "# Forward pass\n",
    "activations = [input]\n",
    "for i, (W, b) in enumerate(zip(weights, biases)):\n",
    "    is_last = (i == len(weights) - 1)\n",
    "    z = W @ activations[-1] + b\n",
    "    activations.append(\n",
    "        z if is_last else torch.relu(z) # skip ReLU for the output layer\n",
    "    )\n",
    "\n",
    "output = activations.pop()\n",
    "\n",
    "# Backpropagation using manual calculations\n",
    "manual_weight_grads = list()\n",
    "manual_bias_grads = list()\n",
    "with torch.no_grad():\n",
    "    loss_grad = 2/len(label) * (output - label) # MSE loss gradient\n",
    "    bias_grad = loss_grad.clone()\n",
    "    for W in reversed(weights):\n",
    "        a = activations.pop()\n",
    "        weight_grad = torch.outer(bias_grad, a)\n",
    "        manual_weight_grads.append(weight_grad.clone())\n",
    "        manual_bias_grads.append(bias_grad.clone())\n",
    "\n",
    "        # for the next iteration\n",
    "        sigma_derivative = (a > 0).to(torch.float32) # ReLU derivative\n",
    "        bias_grad = (W.T @ bias_grad) * sigma_derivative\n",
    "\n",
    "    manual_weight_grads.reverse()\n",
    "    manual_bias_grads.reverse()\n",
    "\n",
    "# Backpropagation using autograd\n",
    "loss = torch.nn.functional.mse_loss(output, label)\n",
    "loss.backward()\n",
    "\n",
    "# Compare results\n",
    "for i, (W, b) in enumerate(zip(weights, biases)):\n",
    "    print(f\"Layer {i+1} weight grad difference: {torch.linalg.vector_norm(W.grad - manual_weight_grads[i])}\")\n",
    "    print(f\"Layer {i+1} bias grad difference: {torch.linalg.vector_norm(b.grad - manual_bias_grads[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
